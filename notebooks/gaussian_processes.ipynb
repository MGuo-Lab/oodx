{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian processes\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook introduces the Gaussian process surrogate modelling implementation in `sumomo`.\n",
    "\n",
    "Begin by ensuring that this notebook directory is on the system path and importing necessary packages including `numpy`. The `sumomo` functionality is made accessible through the `api` instance, whilst utilities such as black box models and plotting functions are imported from the `utils` script. We use the `plot_peaks` function to visualise the underlying black box model used in this notebook for demonstrative purposes, coded in `peaks` and with the functional form shown below.\n",
    "\n",
    "$$ f(x) = 3(1-x_0)^2 \\exp\\left(-x_0^2 - (x_1 + 1)^2\\right) - 10 \\left(\\frac{x_0}{5} - x_0^3 - x_1^5\\right)\\exp\\left(-x_0^2 - x_1^2\\right) - \\frac{1}{3}\\exp\\left(-(x_0+1)^2 - x_1^2\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sumomo import API\n",
    "from utils import peaks, plot_peaks, plot_gp, func_1d, plot_1d\n",
    "\n",
    "\n",
    "plot_peaks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "Initialise the sampling strategy with 100 samples bounded in two dimensions between (-3.0, 3.0) with 1 output variable using Latin hypercube sampling. Sample the output data from the peaks function. Split the data into training (70 samples) and testing sets (30 samples). Standardise the data accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.27407202]\n",
      " [-0.39803534]\n",
      " [-1.34125936]\n",
      " [-0.27473004]\n",
      " [-0.13071783]\n",
      " [-0.27844386]\n",
      " [ 1.94406757]\n",
      " [-0.32233033]\n",
      " [ 2.43904223]\n",
      " [-0.22841099]\n",
      " [ 0.22527201]\n",
      " [-0.28687547]\n",
      " [-1.45156212]\n",
      " [-0.27482411]\n",
      " [ 0.59690628]\n",
      " [-0.50909184]\n",
      " [ 0.55930623]\n",
      " [ 1.35899949]\n",
      " [ 0.20418759]\n",
      " [-0.26794151]\n",
      " [-0.15200935]\n",
      " [ 0.72969249]\n",
      " [-0.26906668]\n",
      " [-1.17076913]\n",
      " [-0.27434173]\n",
      " [-0.27338842]\n",
      " [ 0.63785416]\n",
      " [-0.29378957]\n",
      " [-0.24559238]\n",
      " [-0.28136971]]\n"
     ]
    }
   ],
   "source": [
    "api = API(100, [(-3.0, 3.0), (-3.0, 3.0)], 1, 'lhs')\n",
    "\n",
    "for i in range(api.x.shape[0]):\n",
    "    api.y[i] = peaks(api.x[i])\n",
    "\n",
    "api.split(test_size=0.3)\n",
    "api.scale()\n",
    "\n",
    "print(api.y_test_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process regression\n",
    "\n",
    "A Gaussian process describes a Gaussian (or Normal) distribution over functions. Similar to how a normal distribution of random variables can be fully defined by a mean and variance, a Gaussian process is specified completely by a mean function, $\\mu(x)$, and a covariance function, $k(x, x')$. Specifically, a real process $f(\\textbf{x})$ can be represented by a Gaussian process over functions with a joint distribution shown below. Notice here that the mean and covariance of *functions* are written as functions of *inputs*. We write that the real process is asymptotic to the Gaussian process because, as more observations are made, the distribution over functions becomes tighter around the real function.\n",
    "\n",
    "$$ f(\\textbf{x}) \\sim \\mathcal{N} \\left(\\mu(\\textbf{x}), k(\\textbf{x}, \\textbf{x}') \\right) $$\n",
    "\n",
    "The covariance function enables users to incorporate expert knowledge into the model, for example favouring smooth, periodic, or noisy functions. In this case, the squared exponential covariance function is used, where $\\sigma^2_f$ and $l$ are the postive function variance parameter and characteristic length scale, respectively. The length scale represents the sensitivty of the function, where higher values ensure that even two points far away are correlated resluting in an overall smoothing effect, whilst lower values result in higher frequency function changes. Note that the length scale appears as $l^2$ is the equation as it has been taken outside the squared distance term although it could be written as $ \\left|\\frac{x_j - x'_j}{l}\\right|^2 $. This covariance function can be evaluated between two points $\\textbf{x}, \\textbf{x}'$ in $m$-dimensional space, such that $x_j, x'_j$ represent coordinates for $j=1, ... m$.\n",
    "\n",
    "$$ k(\\textbf{x}, \\textbf{x}') = \\sigma^2_f\\exp\\left( - \\sum_{j=1}^m \\frac{1}{2l^2} \\left|x_j - x'_j\\right|^2 \\right) $$\n",
    "\n",
    "The mean function is typically assumed to a constant or equal to zero since modelling the covariances in this way is sufficient to capture the overall model behaviour. Modelling the mean as a constant removes the dependence on the inputs and in the case of mean equal to zero, the Gaussian process prior distrubtion can be written as below. This 0 assumption has the extra validation that since the data is standardised prior to modelling the mean of the data is actually 0.\n",
    "\n",
    "$$ f(\\textbf{x}) \\sim \\mathcal{N} \\left(0, k(\\textbf{x}, \\textbf{x}') \\right) $$\n",
    "\n",
    "Given a set of training observations $\\textbf{f}$ made over training inputs $\\textbf{x}$, and a single testing input $\\textbf{x}_*$, the joint distribution with the unknown testing output $f_*$ according to the prior is given by below. It is possible to simplify the notation, such that $K$ represents the $n\\times n$ covariance matrix between $n$ training samples, $k(\\textbf{x}_*$ is the $n$-vector of covariances between the new input and $n$ training samples, and the covariance between the new input and itself $k(\\textbf{x}_*, \\textbf{x}_*)$ is equal to the process variance $\\sigma^2_f$.\n",
    "\n",
    "$$ \\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\textbf{f}\\\\\n",
    "f_*\n",
    "\\end{bmatrix} &  \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\textbf{0}\\\\ 0 \\end{bmatrix}, \\begin{bmatrix}\n",
    "K(\\textbf{x}, \\textbf{x}) & \\textbf{k}(\\textbf{x}, \\textbf{x}_*)\\\\\n",
    "\\textbf{k}(\\textbf{x}_*, \\textbf{x}) & k(\\textbf{x}_*, \\textbf{x}_*)\n",
    "\\end{bmatrix} \\right) \\\\\n",
    "& \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\textbf{0}\\\\ 0 \\end{bmatrix}, \\begin{bmatrix}\n",
    "K & \\textbf{k}_*\\\\\n",
    "\\textbf{k}^T_* & \\sigma^2_f\n",
    "\\end{bmatrix} \\right) \\end{align}$$\n",
    "\n",
    "By conditioning this joint distribution prior on the observations,\n",
    "\n",
    "$$ \\begin{bmatrix}\n",
    "\\textbf{x}\\\\\n",
    "\\textbf{y}\n",
    "\\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\boldsymbol{\\mu}_x\\\\ \\boldsymbol{\\mu}_y \\end{bmatrix}, \\begin{bmatrix}\n",
    "A & C\\\\\n",
    "C^T & B\n",
    "\\end{bmatrix} \\right)$$\n",
    "\n",
    "$$ \\textbf{x} | \\textbf{y} \\sim \\mathcal{N} \\left( \\boldsymbol{\\mu}_x + C B^{-1}(\\textbf{y} - \\boldsymbol{\\mu}_y) , A - C B ^{-1} C^T \\right) $$\n",
    "\n",
    "### Predictive function and uncertainty\n",
    "\n",
    "The testing output can be written as a Gaussian posterior distribution with mean function and variance.\n",
    "\n",
    "$$ \\bar{f_*} = \\textbf{k}_*^T K^{-1}\\textbf{f} $$\n",
    "\n",
    "$$ \\mathbb{V}\\left[{f_*}\\right] = \\sigma^2_f - \\textbf{k}_*^T K^{-1}\\textbf{k}_* $$\n",
    "\n",
    "### Predictions with noisy observations\n",
    "\n",
    "In the case of noisy observations where $\\textbf{y} = \\textbf{f} + \\epsilon$, this noise can be incorporated into the covariance function by adding an additional variance term to the diagonal.\n",
    "\n",
    "$$ \\text{cov}(\\textbf{y}) = K(\\textbf{x}, \\textbf{x}) + \\sigma^2_n I $$\n",
    " \n",
    "The corresponding joint distrubtion between noisy observations and a noisy prediction can be written as.\n",
    " \n",
    "$$ \\begin{align}\n",
    "\\begin{bmatrix}\n",
    "\\textbf{y}\\\\\n",
    "y_*\n",
    "\\end{bmatrix} \\sim \\mathcal{N} \\left( \\begin{bmatrix} \\textbf{0}\\\\ 0 \\end{bmatrix}, \\begin{bmatrix}\n",
    "K + \\sigma^2_n I & \\textbf{k}_*\\\\\n",
    "\\textbf{k}^T_* & \\sigma^2_f + \\sigma^2_n\n",
    "\\end{bmatrix} \\right) \\end{align}$$\n",
    "\n",
    "The resulting predictive mean function and variance function provide a more generalised version with optional noise.\n",
    "\n",
    "$$ \\bar{y_*} = \\textbf{k}_*^T (K + \\sigma^2_n I)^{-1}\\textbf{y} $$\n",
    "\n",
    "$$ \\mathbb{V}\\left[{y_*}\\right] = \\sigma^2_f + \\sigma^2_n - \\textbf{k}_*^T (K + \\sigma^2_n I)^{-1}\\textbf{k}_* $$\n",
    "\n",
    "### Linear predictor\n",
    "\n",
    "Since $(K + \\sigma^2_n I)^{-1}\\textbf{y}$ is only a function of the training data, only $\\textbf{k}_*^T$ depends on the new input, so the predictive mean function can be written as the linear predictor.\n",
    "\n",
    "$$ \\bar{y}(\\textbf{x}_*) = \\sum_i^n \\alpha_i k(\\textbf{x}_i, \\textbf{x}_* ) $$\n",
    " \n",
    " ### GPR\n",
    " \n",
    "The Sumomo `GPR` class inherits from the `GaussianProcessRegressor` from *scikit-learn*. Specifically, `GPR` is initialised with the an optional `noise` attribute with default value equal to 0. Additionally, `GPR` is initialised with the squared exponential kernel also known as the radial basis function `RBF` kernel from *scikit-learn*, with an initial `length_scale` equal to 1 and `length_scale_bounds` at (0, 100). The `1.0` pre-factor to `RBF` provides a kernel scaling paramter equivalent to $\\sigma_f^2$.\n",
    "\n",
    "`GPR` has a method `fit(x, y)` which calls the method of the same name from `GaussianProcessRegressor` whilst simultaneously saving `x` and `y`, and the optimisation parameters to the class for further use. The fitting is performed by maximum likelihood estimation to minimise the negative log marginal likelihood using *scipy.optimize.minimize*.\n",
    "\n",
    "Specifically, the `GPR` class method `save_params()` obtains the optimised kernel parameters after fitting, the pre-exponential `constant_value` equivalent to $\\sigma_f^2$ and the `length_scale`. In addition, `invK` equivalent to $K^{-1}$ and `alpha` from the linear predictor are saved. Finally, further consolidation is done to save $\\alpha_i \\sigma_f^2$ as an $n$-vector `beta` where $n$ is the number of training samples and size of $\\alpha$.\n",
    "\n",
    "Predictions are made using `GPR.predict(x)` with optional parameter `return_std` equal to `False` in which case only the predictive mean function is evaluated at the inputs. For `return_std` equal to `True`, the predictive mean is returned along with the standard deviation of the predictions, equivalent to $\\sqrt{\\mathbb{V}\\left[{y_*}\\right]}$ Again, the `GaussianProcessRegressor` predict method is called.\n",
    "\n",
    "Finally, `GPR` contained the `formulation(x, return_std=False)` method with the same function as `predict()` but with *NumPy* formulations equivalent to the *Pyomo* fomrulations implemented later.\n",
    "\n",
    "```python\n",
    "class GPR(GaussianProcessRegressor):\n",
    "    def __init__(self, noise=0.0):\n",
    "        super().__init__(kernel=self.kernel(), alpha=noise)\n",
    "        self.name = 'GPR'\n",
    "        self.x_train = None\n",
    "        self.length_scale = None\n",
    "        self.constant_value = None\n",
    "        self.inv_K = None\n",
    "        self.noise = noise\n",
    "    \n",
    "    def kernel(self):\n",
    "        kernel = 1.0 * RBF(length_scale=1.0, length_scale_bounds=(0, 1e2))\n",
    "        return kernel\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        self.x_train = x\n",
    "        super().fit(x, y)\n",
    "        self.save_params()\n",
    "    \n",
    "    def save_params(self):\n",
    "        params = self.kernel_.get_params()\n",
    "        self.constant_value = params['k1__constant_value']\n",
    "        self.length_scale = params['k2__length_scale']\n",
    "        self.alpha = self.alpha_.ravel()\n",
    "        K = self.kernel_(self.x_train, self.x_train) + np.eye(self.x_train.shape[0]) * self.noise\n",
    "        self.inv_K = inv(K)\n",
    "    \n",
    "    def predict(self, x, return_std=False):\n",
    "        if return_std:\n",
    "            return super().predict(x, return_std=True)\n",
    "        else:\n",
    "            return super().predict(x, return_std=False)\n",
    "\n",
    "    def formulation(self, x, return_std=False):\n",
    "        n = self.x_train.shape[0]\n",
    "        m = self.x_train.shape[1]\n",
    "        sq_exp = np.exp(\n",
    "            -sum(0.5 / self.length_scale ** 2 * (x[:, j] - self.x_train[:, j]) ** 2 for j in range(m))\n",
    "            )\n",
    "        k_s = self.constant_value * sq_exp\n",
    "        pred = sum(k_s[i] * self.alpha[i] for i in range(n))\n",
    "        if return_std:\n",
    "            k_K_k = sum(k_s[i] * sum(self.inv_K[i, j] * k_s[j] for j in range(n)) for i in range(n))\n",
    "            var = self.constant_value + self.noise - k_K_k\n",
    "            std = np.sqrt(var)\n",
    "            return pred, std\n",
    "        else:\n",
    "            return pred\n",
    "```\n",
    "\n",
    "\n",
    "* varying $l$\n",
    "* varying $\\sigma^2_f$\n",
    "* varying $\\sigma^2_n$\n",
    "* different kernel functions\n",
    "* fitting or parameter optimisation\n",
    "* *NumPy* formulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.initialise(3, [(-8.0, 8.0)], 1, 'lhs')\n",
    "\n",
    "for i in range(api.x.shape[0]):\n",
    "    api.y[i] = func_1d(api.x[i])\n",
    "\n",
    "# api.split(test_size=0.3)\n",
    "api.scale()\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.init_regressor('gp', noise=0.0)\n",
    "\n",
    "with np.errstate(divide='ignore'):\n",
    "    api.regressor.fit(api.x_train_, api.y_train_)\n",
    "\n",
    "plot_gp(api, show_samples=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api.test('rmse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(api.regressor.length_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian process classification\n",
    "\n",
    "Implemented in *NumPy*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sumomo.gp import GPC\n",
    "\n",
    "\n",
    "X_train, t = make_moons(50, noise=0.3)\n",
    "t = t.reshape(-1, 1)\n",
    "\n",
    "gpc = GPC()\n",
    "gpc.fit(X_train, t)\n",
    "print(gpc.l, gpc.kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2 = np.linspace(-3, 3, 50), np.linspace(-3, 3, 50)\n",
    "x1grid, x2grid = np.meshgrid(x1, x2)\n",
    "X = np.c_[x1grid.ravel(), x2grid.ravel()]\n",
    "\n",
    "prediction, std = gpc.predict(X, return_std=True)\n",
    "print(prediction)\n",
    "formulation_pred = np.zeros_like(prediction)\n",
    "for i in range(len(formulation_pred)):\n",
    "    formulation_pred[i] = gpc.formulation(X[i])\n",
    "print(formulation_pred)\n",
    "prediction = prediction.reshape(x1grid.shape)\n",
    "std = std.reshape(x1grid.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax1 = fig.add_subplot(111)\n",
    "c1 = ax1.contourf(x1, x2, prediction, levels=12, alpha=0.8)\n",
    "ax1.scatter(X_train[:, 0], X_train[:, 1], s=10, c=t, cmap='bwr_r')\n",
    "ax1.contour(x1, x2, prediction, levels=[0.2, 0.5, 0.8], linestyles='dashed', colors='k', linewidths=1)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "fig.colorbar(c1, ax=ax1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax1 = fig.add_subplot(111)\n",
    "c1 = ax1.contourf(x1, x2, std, levels=12, alpha=0.8)\n",
    "ax1.scatter(X_train[:, 0], X_train[:, 1], s=10, c=t, cmap='bwr_r')\n",
    "ax1.contour(x1, x2, prediction, levels=[0.2, 0.5, 0.8], linestyles='dashed', colors='k', linewidths=1)\n",
    "ax1.set_xlabel('$x_1$')\n",
    "ax1.set_ylabel('$x_2$')\n",
    "fig.colorbar(c1, ax=ax1)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = [(-6.0, 6.0)]\n",
    "n_samples = 11\n",
    "\n",
    "from sumomo.gp import GPR\n",
    "\n",
    "x_train = api.initialise(n_samples, space)\n",
    "\n",
    "y_train = func_1d(x_train)\n",
    "\n",
    "t = np.ones_like(x_train)\n",
    "t[x_train < -4] = 0\n",
    "t[x_train > 2] = 0\n",
    "t = t.reshape(-1, 1)\n",
    "\n",
    "gpr = GPR()\n",
    "gpr.fit(x_train[t.ravel()==1, :], y_train[t.ravel()==1])\n",
    "\n",
    "gpc = GPC()\n",
    "gpc.fit(x_train, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = np.arange(-8, 8, 0.01).reshape(-1, 1)\n",
    "pred, std = gpr.predict(x_new, return_std=True)\n",
    "un = 1.96 * std\n",
    "f, s = gpc.predict(x_new, return_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_new, pred)\n",
    "\n",
    "ax.plot(x_new, func_1d(x_new), ls='--', alpha=0.5, c='k')\n",
    "ax.title.set_text('gpr predictions')\n",
    "ax.fill_between(x_new.ravel(), pred.ravel() + un.ravel(), pred.ravel() - un.ravel(), alpha=0.2)\n",
    "ax.scatter(x_train[t.ravel()==1, :], y_train[t.ravel()==1], c='b')\n",
    "ax.scatter(x_train[t.ravel()==0, :], y_train[t.ravel()==0], c='r')\n",
    "\n",
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_new, f, ls='--')\n",
    "ax.scatter(x_train[t==1], t[t==1], c='b')\n",
    "ax.scatter(x_train[t==0], t[t==0], c='r')\n",
    "ax.title.set_text('gpc predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(4, 3))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(x_new, s, ls='--')\n",
    "ax.title.set_text('gpc uncertainty')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "509b8fb1f9bb9a36cf455cc3b6d7786e56047ba93e78ea1e04340272261a598e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
